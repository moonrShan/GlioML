{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa55bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.common.utils.utils import setup_outputdir\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "from autogluon.core.utils.savers import save_pkl\n",
    "import os.path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41b72ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilabelPredictor():\n",
    "    \"\"\" Tabular Predictor for predicting multiple columns in table.\n",
    "        Creates multiple TabularPredictor objects which you can also use individually.\n",
    "        You can access the TabularPredictor for a particular label via: `multilabel_predictor.get_predictor(label_i)`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List[str]\n",
    "            The ith element of this list is the column (i.e. `label`) predicted by the ith TabularPredictor stored in this object.\n",
    "        path : str, default = None\n",
    "            Path to directory where models and intermediate outputs should be saved.\n",
    "            If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n",
    "            Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n",
    "            Otherwise files from first `fit()` will be overwritten by second `fit()`.\n",
    "            Caution: when predicting many labels, this directory may grow large as it needs to store many TabularPredictors.\n",
    "        problem_types : List[str], default = None\n",
    "            The ith element is the `problem_type` for the ith TabularPredictor stored in this object.\n",
    "        eval_metrics : List[str], default = None\n",
    "            The ith element is the `eval_metric` for the ith TabularPredictor stored in this object.\n",
    "        consider_labels_correlation : bool, default = True\n",
    "            Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others.\n",
    "            If True, the ordering of `labels` may affect resulting accuracy as each label is predicted conditional on the previous labels appearing earlier in this list (i.e. in an auto-regressive fashion).\n",
    "            Set to False if during inference you may want to individually use just the ith TabularPredictor without predicting all the other labels.\n",
    "        kwargs :\n",
    "            Arguments passed into the initialization of each TabularPredictor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    multi_predictor_file = 'multilabel_predictor.pkl'\n",
    "\n",
    "    def __init__(self, labels, path=None, problem_types=None, eval_metrics=None, consider_labels_correlation=False, **kwargs):\n",
    "        if (problem_types is not None) and (len(problem_types) != len(labels)):\n",
    "            raise ValueError(\"If provided, `problem_types` must have same length as `labels`\")\n",
    "        if (eval_metrics is not None) and (len(eval_metrics) != len(labels)):\n",
    "            raise ValueError(\"If provided, `eval_metrics` must have same length as `labels`\")\n",
    "        self.path = setup_outputdir(path, warn_if_exist=False)\n",
    "        self.labels = labels\n",
    "        self.consider_labels_correlation = consider_labels_correlation\n",
    "        self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n",
    "        if eval_metrics is None:\n",
    "            self.eval_metrics = {}\n",
    "        else:\n",
    "            self.eval_metrics = {labels[i] : eval_metrics[i] for i in range(len(labels))}\n",
    "        problem_type = None\n",
    "        eval_metric = None\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            path_i = self.path + \"Predictor_\" + label\n",
    "            if problem_types is not None:\n",
    "                problem_type = problem_types[i]\n",
    "            if eval_metrics is not None:\n",
    "                eval_metric = eval_metrics[i]\n",
    "            self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric, path=path_i, **kwargs)\n",
    "\n",
    "    def fit(self, train_data, tuning_data=None, final = False, **kwargs):\n",
    "        \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_data, tuning_data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                See documentation for `TabularPredictor.fit()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `fit()` call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        if isinstance(train_data, str):\n",
    "            train_data = TabularDataset(train_data)\n",
    "        if tuning_data is not None and isinstance(tuning_data, str):\n",
    "            tuning_data = TabularDataset(tuning_data)\n",
    "        train_data_og = train_data.copy()\n",
    "        if tuning_data is not None:\n",
    "            tuning_data_og = tuning_data.copy()\n",
    "        else:\n",
    "            tuning_data_og = None\n",
    "        save_metrics = len(self.eval_metrics) == 0\n",
    "        start = time.time()\n",
    "        for i in range(len(self.labels)):\n",
    "            label = self.labels[i]\n",
    "            predictor = self.get_predictor(label)\n",
    "            if not self.consider_labels_correlation:\n",
    "                labels_to_drop = [l for l in self.labels if l != label]\n",
    "            else:\n",
    "                labels_to_drop = [self.labels[j] for j in range(i+1, len(self.labels))]\n",
    "            train_data = train_data_og.drop(labels_to_drop, axis=1)\n",
    "            if tuning_data is not None:\n",
    "                tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n",
    "            print(f\"Fitting TabularPredictor for label: {label} ...{i / len(self.labels) * 100}%\")\n",
    "            print(f\"{(time.time() - start) / 60} minutes\")\n",
    "            if (final):\n",
    "                predictor.fit(train_data=train_data[train_data[label] > float('-inf')]\n",
    "                              , tuning_data = tuning_data\n",
    "                              ,presets = 'best_quality'\n",
    "                              ,num_bag_folds = 5,num_bag_sets = 2\n",
    "                              , **kwargs)\n",
    "            else:\n",
    "                predictor.fit(train_data=train_data[train_data[label] > float('-inf')]\n",
    "                              , tuning_data = tuning_data\n",
    "                              ,presets = 'medium_quality'\n",
    "                              #,presets = 'best_quality'\n",
    "                              #,num_bag_folds = 5,num_bag_sets = 2\n",
    "                              , **kwargs)\n",
    "            self.predictors[label] = predictor.path\n",
    "            if save_metrics:\n",
    "                self.eval_metrics[label] = predictor.eval_metric\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, data, **kwargs):\n",
    "        \"\"\" Returns DataFrame with label columns containing predictions for each label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. If label columns are present in this data, they will be ignored. See documentation for `TabularPredictor.predict()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the predict() call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=False, **kwargs)\n",
    "\n",
    "    def predict_proba(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=True, **kwargs)\n",
    "\n",
    "    def evaluate(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n",
    "        \"\"\"\n",
    "        data = self._get_data(data)\n",
    "        eval_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Evaluating TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            \n",
    "            eval_dict[label] = predictor.evaluate(data[data[label] > float('-inf')], **kwargs)\n",
    "            if self.consider_labels_correlation:\n",
    "                data[label] = predictor.predict(data, **kwargs)\n",
    "        return eval_dict\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" Save MultilabelPredictor to disk. \"\"\"\n",
    "        for label in self.labels:\n",
    "            if not isinstance(self.predictors[label], str):\n",
    "                self.predictors[label] = self.predictors[label].path\n",
    "        save_pkl.save(path=self.path+self.multi_predictor_file, object=self)\n",
    "        print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n",
    "        path = os.path.expanduser(path)\n",
    "        if path[-1] != os.path.sep:\n",
    "            path = path + os.path.sep\n",
    "        return load_pkl.load(path=path+cls.multi_predictor_file)\n",
    "\n",
    "    def get_predictor(self, label):\n",
    "        \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n",
    "        predictor = self.predictors[label]\n",
    "        if isinstance(predictor, str):\n",
    "            return TabularPredictor.load(path=predictor)\n",
    "        return predictor\n",
    "\n",
    "    def _get_data(self, data):\n",
    "        if isinstance(data, str):\n",
    "            return TabularDataset(data)\n",
    "        return data.copy()\n",
    "\n",
    "    def _predict(self, data, as_proba=False, **kwargs):\n",
    "        data = self._get_data(data)\n",
    "        if as_proba:\n",
    "            predproba_dict = {}\n",
    "        for i,label in enumerate(self.labels):\n",
    "            print(f\"Predicting with TabularPredictor for label: {label} ...{i / len(self.labels) * 100}%\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            if as_proba:\n",
    "                predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n",
    "            data[label] = predictor.predict(data, **kwargs)\n",
    "        if not as_proba:\n",
    "            return data[self.labels]\n",
    "        else:\n",
    "            return predproba_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3ae94b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-02T06:09:54.216156Z",
     "start_time": "2022-07-02T06:09:54.210855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n",
      "187\n",
      "197\n",
      "1616\n",
      "665\n",
      "280\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "#1 Load my ccl's ssGSEA signature\n",
    "myCCLSignature = []\n",
    "for name in ['sample.c2.cp.biocarta.gct',\n",
    "             'sample.c2.cp.kegg.gct',\n",
    "             'sample.c2.cp.pid.gct',\n",
    "             'sample.c2.cp.reactome.gct',\n",
    "             'sample.c2.cp.wiki.gct',\n",
    "             'sample.c6.gct',\n",
    "             'sample.hallmark.gct']:    \n",
    "    with open(name, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        CCLSignature = list(csvFile)[2:]\n",
    "        print(len(CCLSignature))\n",
    "    for i, row in enumerate(CCLSignature):\n",
    "        temp = CCLSignature[i][0].split('\\t')\n",
    "        if i > 0:\n",
    "            CCLSignature[i] = [temp[0]] + [float(d) for d in temp[2:]]\n",
    "        else:\n",
    "            CCLSignature[i] = [temp[0]] + temp[2:]\n",
    "    if not myCCLSignature:\n",
    "        myCCLSignature += CCLSignature\n",
    "    else:\n",
    "        myCCLSignature += CCLSignature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1ebef18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-02T06:10:01.747913Z",
     "start_time": "2022-07-02T06:10:01.683560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n",
      "187\n",
      "197\n",
      "1616\n",
      "665\n",
      "280\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "#2 Load CCLE ssGSEA signature\n",
    "CCLECCLSignature = []\n",
    "for name in ['ccle.c2.cp.biocarta.gct',\n",
    "             'ccle.c2.cp.kegg.gct',\n",
    "             'ccle.c2.cp.pid.gct',\n",
    "             'ccle.c2.cp.reactome.gct',\n",
    "             'ccle.c2.cp.wiki.gct',\n",
    "             'ccle.c6.gct',\n",
    "             'ccle.hallmark.gct']:\n",
    "    with open(name, mode ='r')as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        CCLSignature = list(csvFile)[2:]\n",
    "        print(len(CCLSignature))\n",
    "    for i, row in enumerate(CCLSignature):\n",
    "        temp = CCLSignature[i][0].split('\\t')\n",
    "        CCLSignature[i] = [temp[0]] + temp[2:]\n",
    "    if not CCLECCLSignature:\n",
    "        CCLECCLSignature += CCLSignature\n",
    "    else:\n",
    "        CCLECCLSignature += CCLSignature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce386d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-02T06:10:47.896499Z",
     "start_time": "2022-07-02T06:10:47.530996Z"
    }
   },
   "outputs": [],
   "source": [
    "#3 Load CTRP cclName to AUC map\n",
    "cclToAUCdict = collections.defaultdict(list)\n",
    "with open('CTRP_CCL_AUC.gct', mode ='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    CTRPCCLAUC = list(csvFile)\n",
    "    CTRPCCLAUC = [''.join(sub).split('\\t') for sub in CTRPCCLAUC]\n",
    "    cclNames = CTRPCCLAUC[3][4:]\n",
    "\n",
    "for i,cclName in enumerate(cclNames):\n",
    "    cclToAUCdict[cclName] = [float( '-inf' if sub[4+i] == 'NaN' else sub[4+i]) for sub in CTRPCCLAUC[7:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfc80f9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-02T06:10:57.926109Z",
     "start_time": "2022-07-02T06:10:57.915764Z"
    }
   },
   "outputs": [],
   "source": [
    "#4 Load ccleID to ctrpName map\n",
    "CCLEidToCTRPNameDict = collections.defaultdict(str)\n",
    "CCLEidToDiseaseName = collections.defaultdict(str)\n",
    "with open('sample_info.csv', mode ='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    mapInfos = list(csvFile)\n",
    "    for mapInfo in mapInfos[1:]:\n",
    "        CCLEidToCTRPNameDict[mapInfo[0]] = mapInfo[2]  \n",
    "        CCLEidToDiseaseName[mapInfo[0]] = mapInfo[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8996ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc53a03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIOCARTA_GRANULOCYTES_PATHWAY</th>\n",
       "      <th>BIOCARTA_LYM_PATHWAY</th>\n",
       "      <th>BIOCARTA_BLYMPHOCYTE_PATHWAY</th>\n",
       "      <th>BIOCARTA_CARM_ER_PATHWAY</th>\n",
       "      <th>BIOCARTA_LAIR_PATHWAY</th>\n",
       "      <th>BIOCARTA_VDR_PATHWAY</th>\n",
       "      <th>BIOCARTA_MTA3_PATHWAY</th>\n",
       "      <th>BIOCARTA_GABA_PATHWAY</th>\n",
       "      <th>BIOCARTA_EGFR_SMRTE_PATHWAY</th>\n",
       "      <th>BIOCARTA_MONOCYTE_PATHWAY</th>\n",
       "      <th>...</th>\n",
       "      <th>HALLMARK_COAGULATION</th>\n",
       "      <th>HALLMARK_IL2_STAT5_SIGNALING</th>\n",
       "      <th>HALLMARK_BILE_ACID_METABOLISM</th>\n",
       "      <th>HALLMARK_PEROXISOME</th>\n",
       "      <th>HALLMARK_ALLOGRAFT_REJECTION</th>\n",
       "      <th>HALLMARK_SPERMATOGENESIS</th>\n",
       "      <th>HALLMARK_KRAS_SIGNALING</th>\n",
       "      <th>HALLMARK_KRAS_SIGNALING_UP</th>\n",
       "      <th>HALLMARK_KRAS_SIGNALING_DN</th>\n",
       "      <th>HALLMARK_PANCREAS_BETA_CELLS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>THP1_d3_B.TPM</th>\n",
       "      <td>0.991211</td>\n",
       "      <td>0.985279</td>\n",
       "      <td>0.984154</td>\n",
       "      <td>0.876359</td>\n",
       "      <td>0.993570</td>\n",
       "      <td>0.595547</td>\n",
       "      <td>0.475054</td>\n",
       "      <td>0.492397</td>\n",
       "      <td>0.594030</td>\n",
       "      <td>0.986427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675939</td>\n",
       "      <td>0.809839</td>\n",
       "      <td>0.680037</td>\n",
       "      <td>0.774811</td>\n",
       "      <td>0.928606</td>\n",
       "      <td>0.793392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880986</td>\n",
       "      <td>0.337452</td>\n",
       "      <td>0.670398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2D_1.TPM</th>\n",
       "      <td>0.160241</td>\n",
       "      <td>0.161473</td>\n",
       "      <td>0.059402</td>\n",
       "      <td>0.379321</td>\n",
       "      <td>0.045364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089837</td>\n",
       "      <td>0.554072</td>\n",
       "      <td>0.363435</td>\n",
       "      <td>0.187854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.065203</td>\n",
       "      <td>0.221749</td>\n",
       "      <td>0.194506</td>\n",
       "      <td>0.019127</td>\n",
       "      <td>0.555587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037313</td>\n",
       "      <td>0.343351</td>\n",
       "      <td>0.248108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRId7CWa.quant.TPM</th>\n",
       "      <td>0.438166</td>\n",
       "      <td>0.443539</td>\n",
       "      <td>0.540397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506852</td>\n",
       "      <td>0.304020</td>\n",
       "      <td>0.583516</td>\n",
       "      <td>0.429940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927243</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.484722</td>\n",
       "      <td>0.130770</td>\n",
       "      <td>0.436573</td>\n",
       "      <td>0.053547</td>\n",
       "      <td>0.354081</td>\n",
       "      <td>0.771492</td>\n",
       "      <td>0.968649</td>\n",
       "      <td>0.720934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H7.TPM</th>\n",
       "      <td>0.553921</td>\n",
       "      <td>0.562553</td>\n",
       "      <td>0.428064</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724494</td>\n",
       "      <td>0.403809</td>\n",
       "      <td>0.852908</td>\n",
       "      <td>0.493875</td>\n",
       "      <td>0.511236</td>\n",
       "      <td>0.531963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822794</td>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.113695</td>\n",
       "      <td>0.248351</td>\n",
       "      <td>0.476935</td>\n",
       "      <td>0.189215</td>\n",
       "      <td>0.544488</td>\n",
       "      <td>0.707475</td>\n",
       "      <td>0.643843</td>\n",
       "      <td>0.258149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THP1_d3_A.TPM</th>\n",
       "      <td>0.992471</td>\n",
       "      <td>0.992554</td>\n",
       "      <td>0.973644</td>\n",
       "      <td>0.795964</td>\n",
       "      <td>0.999802</td>\n",
       "      <td>0.548292</td>\n",
       "      <td>0.615737</td>\n",
       "      <td>0.447267</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.989311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691364</td>\n",
       "      <td>0.826434</td>\n",
       "      <td>0.710767</td>\n",
       "      <td>0.877729</td>\n",
       "      <td>0.928190</td>\n",
       "      <td>0.791363</td>\n",
       "      <td>0.947641</td>\n",
       "      <td>0.933168</td>\n",
       "      <td>0.476588</td>\n",
       "      <td>0.819935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0                   BIOCARTA_GRANULOCYTES_PATHWAY  BIOCARTA_LYM_PATHWAY  \\\n",
       "Name                                                                      \n",
       "THP1_d3_B.TPM                            0.991211              0.985279   \n",
       "2D_1.TPM                                 0.160241              0.161473   \n",
       "TRId7CWa.quant.TPM                       0.438166              0.443539   \n",
       "H7.TPM                                   0.553921              0.562553   \n",
       "THP1_d3_A.TPM                            0.992471              0.992554   \n",
       "\n",
       "0                   BIOCARTA_BLYMPHOCYTE_PATHWAY  BIOCARTA_CARM_ER_PATHWAY  \\\n",
       "Name                                                                         \n",
       "THP1_d3_B.TPM                           0.984154                  0.876359   \n",
       "2D_1.TPM                                0.059402                  0.379321   \n",
       "TRId7CWa.quant.TPM                      0.540397                  0.000000   \n",
       "H7.TPM                                  0.428064                  1.000000   \n",
       "THP1_d3_A.TPM                           0.973644                  0.795964   \n",
       "\n",
       "0                   BIOCARTA_LAIR_PATHWAY  BIOCARTA_VDR_PATHWAY  \\\n",
       "Name                                                              \n",
       "THP1_d3_B.TPM                    0.993570              0.595547   \n",
       "2D_1.TPM                         0.045364              1.000000   \n",
       "TRId7CWa.quant.TPM               0.506852              0.304020   \n",
       "H7.TPM                           0.724494              0.403809   \n",
       "THP1_d3_A.TPM                    0.999802              0.548292   \n",
       "\n",
       "0                   BIOCARTA_MTA3_PATHWAY  BIOCARTA_GABA_PATHWAY  \\\n",
       "Name                                                               \n",
       "THP1_d3_B.TPM                    0.475054               0.492397   \n",
       "2D_1.TPM                         0.089837               0.554072   \n",
       "TRId7CWa.quant.TPM               0.583516               0.429940   \n",
       "H7.TPM                           0.852908               0.493875   \n",
       "THP1_d3_A.TPM                    0.615737               0.447267   \n",
       "\n",
       "0                   BIOCARTA_EGFR_SMRTE_PATHWAY  BIOCARTA_MONOCYTE_PATHWAY  \\\n",
       "Name                                                                         \n",
       "THP1_d3_B.TPM                          0.594030                   0.986427   \n",
       "2D_1.TPM                               0.363435                   0.187854   \n",
       "TRId7CWa.quant.TPM                     1.000000                   0.700186   \n",
       "H7.TPM                                 0.511236                   0.531963   \n",
       "THP1_d3_A.TPM                          0.474747                   0.989311   \n",
       "\n",
       "0                   ...  HALLMARK_COAGULATION  HALLMARK_IL2_STAT5_SIGNALING  \\\n",
       "Name                ...                                                       \n",
       "THP1_d3_B.TPM       ...              0.675939                      0.809839   \n",
       "2D_1.TPM            ...              0.025288                      0.065203   \n",
       "TRId7CWa.quant.TPM  ...              0.927243                      0.736486   \n",
       "H7.TPM              ...              0.822794                      0.816438   \n",
       "THP1_d3_A.TPM       ...              0.691364                      0.826434   \n",
       "\n",
       "0                   HALLMARK_BILE_ACID_METABOLISM  HALLMARK_PEROXISOME  \\\n",
       "Name                                                                     \n",
       "THP1_d3_B.TPM                            0.680037             0.774811   \n",
       "2D_1.TPM                                 0.221749             0.194506   \n",
       "TRId7CWa.quant.TPM                       0.484722             0.130770   \n",
       "H7.TPM                                   0.113695             0.248351   \n",
       "THP1_d3_A.TPM                            0.710767             0.877729   \n",
       "\n",
       "0                   HALLMARK_ALLOGRAFT_REJECTION  HALLMARK_SPERMATOGENESIS  \\\n",
       "Name                                                                         \n",
       "THP1_d3_B.TPM                           0.928606                  0.793392   \n",
       "2D_1.TPM                                0.019127                  0.555587   \n",
       "TRId7CWa.quant.TPM                      0.436573                  0.053547   \n",
       "H7.TPM                                  0.476935                  0.189215   \n",
       "THP1_d3_A.TPM                           0.928190                  0.791363   \n",
       "\n",
       "0                   HALLMARK_KRAS_SIGNALING  HALLMARK_KRAS_SIGNALING_UP  \\\n",
       "Name                                                                      \n",
       "THP1_d3_B.TPM                      1.000000                    0.880986   \n",
       "2D_1.TPM                           0.000000                    0.037313   \n",
       "TRId7CWa.quant.TPM                 0.354081                    0.771492   \n",
       "H7.TPM                             0.544488                    0.707475   \n",
       "THP1_d3_A.TPM                      0.947641                    0.933168   \n",
       "\n",
       "0                   HALLMARK_KRAS_SIGNALING_DN  HALLMARK_PANCREAS_BETA_CELLS  \n",
       "Name                                                                          \n",
       "THP1_d3_B.TPM                         0.337452                      0.670398  \n",
       "2D_1.TPM                              0.343351                      0.248108  \n",
       "TRId7CWa.quant.TPM                    0.968649                      0.720934  \n",
       "H7.TPM                                0.643843                      0.258149  \n",
       "THP1_d3_A.TPM                         0.476588                      0.819935  \n",
       "\n",
       "[5 rows x 3284 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction data \n",
    "predictData = pd.DataFrame(data = myCCLSignature).transpose()\n",
    "new_header = predictData.iloc[0] \n",
    "predictData = predictData[1:] \n",
    "predictData.columns = new_header \n",
    "predictData = predictData.apply(pd.to_numeric, errors='ignore')\n",
    "predictData = predictData.set_index(['Name'])\n",
    "predictData = normalize(predictData)\n",
    "predictData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6935a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train set\n",
    "trainData = pd.DataFrame(data = CCLECCLSignature).transpose()\n",
    "new_header = trainData.iloc[0] \n",
    "trainData = trainData[1:] \n",
    "trainData.columns = new_header "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52014506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIOCARTA_GRANULOCYTES_PATHWAY</th>\n",
       "      <th>BIOCARTA_LYM_PATHWAY</th>\n",
       "      <th>BIOCARTA_BLYMPHOCYTE_PATHWAY</th>\n",
       "      <th>BIOCARTA_CARM_ER_PATHWAY</th>\n",
       "      <th>BIOCARTA_LAIR_PATHWAY</th>\n",
       "      <th>BIOCARTA_VDR_PATHWAY</th>\n",
       "      <th>BIOCARTA_MTA3_PATHWAY</th>\n",
       "      <th>BIOCARTA_GABA_PATHWAY</th>\n",
       "      <th>BIOCARTA_EGFR_SMRTE_PATHWAY</th>\n",
       "      <th>BIOCARTA_MONOCYTE_PATHWAY</th>\n",
       "      <th>...</th>\n",
       "      <th>HALLMARK_COAGULATION</th>\n",
       "      <th>HALLMARK_IL2_STAT5_SIGNALING</th>\n",
       "      <th>HALLMARK_BILE_ACID_METABOLISM</th>\n",
       "      <th>HALLMARK_PEROXISOME</th>\n",
       "      <th>HALLMARK_ALLOGRAFT_REJECTION</th>\n",
       "      <th>HALLMARK_SPERMATOGENESIS</th>\n",
       "      <th>HALLMARK_KRAS_SIGNALING</th>\n",
       "      <th>HALLMARK_KRAS_SIGNALING_UP</th>\n",
       "      <th>HALLMARK_KRAS_SIGNALING_DN</th>\n",
       "      <th>HALLMARK_PANCREAS_BETA_CELLS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACH-001113</th>\n",
       "      <td>0.201899</td>\n",
       "      <td>0.214778</td>\n",
       "      <td>0.107506</td>\n",
       "      <td>0.436254</td>\n",
       "      <td>0.251323</td>\n",
       "      <td>0.543866</td>\n",
       "      <td>0.402230</td>\n",
       "      <td>0.446239</td>\n",
       "      <td>0.692287</td>\n",
       "      <td>0.299681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369956</td>\n",
       "      <td>0.589361</td>\n",
       "      <td>0.231132</td>\n",
       "      <td>0.508302</td>\n",
       "      <td>0.178205</td>\n",
       "      <td>0.653705</td>\n",
       "      <td>0.345327</td>\n",
       "      <td>0.352541</td>\n",
       "      <td>0.390359</td>\n",
       "      <td>0.232997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-000242</th>\n",
       "      <td>0.210679</td>\n",
       "      <td>0.195089</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.598515</td>\n",
       "      <td>0.192437</td>\n",
       "      <td>0.888515</td>\n",
       "      <td>0.381405</td>\n",
       "      <td>0.108177</td>\n",
       "      <td>0.679700</td>\n",
       "      <td>0.285389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448744</td>\n",
       "      <td>0.651476</td>\n",
       "      <td>0.461799</td>\n",
       "      <td>0.710828</td>\n",
       "      <td>0.337337</td>\n",
       "      <td>0.380484</td>\n",
       "      <td>0.503869</td>\n",
       "      <td>0.462545</td>\n",
       "      <td>0.288830</td>\n",
       "      <td>0.128070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-000327</th>\n",
       "      <td>0.226109</td>\n",
       "      <td>0.228415</td>\n",
       "      <td>0.131664</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>0.105343</td>\n",
       "      <td>0.366447</td>\n",
       "      <td>0.534534</td>\n",
       "      <td>0.508604</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377387</td>\n",
       "      <td>0.627130</td>\n",
       "      <td>0.639671</td>\n",
       "      <td>0.951057</td>\n",
       "      <td>0.237352</td>\n",
       "      <td>0.491974</td>\n",
       "      <td>0.441105</td>\n",
       "      <td>0.435839</td>\n",
       "      <td>0.353783</td>\n",
       "      <td>0.339955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-000461</th>\n",
       "      <td>0.230496</td>\n",
       "      <td>0.359472</td>\n",
       "      <td>0.087643</td>\n",
       "      <td>0.529038</td>\n",
       "      <td>0.244749</td>\n",
       "      <td>0.476536</td>\n",
       "      <td>0.201961</td>\n",
       "      <td>0.313338</td>\n",
       "      <td>0.724363</td>\n",
       "      <td>0.298408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475189</td>\n",
       "      <td>0.725638</td>\n",
       "      <td>0.126653</td>\n",
       "      <td>0.531472</td>\n",
       "      <td>0.348210</td>\n",
       "      <td>0.451362</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.517533</td>\n",
       "      <td>0.191123</td>\n",
       "      <td>0.196242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-000792</th>\n",
       "      <td>0.394568</td>\n",
       "      <td>0.663835</td>\n",
       "      <td>0.308974</td>\n",
       "      <td>0.400182</td>\n",
       "      <td>0.578346</td>\n",
       "      <td>0.287388</td>\n",
       "      <td>0.289313</td>\n",
       "      <td>0.165095</td>\n",
       "      <td>0.557640</td>\n",
       "      <td>0.630891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624730</td>\n",
       "      <td>0.829904</td>\n",
       "      <td>0.180853</td>\n",
       "      <td>0.480622</td>\n",
       "      <td>0.450919</td>\n",
       "      <td>0.481082</td>\n",
       "      <td>0.591145</td>\n",
       "      <td>0.458782</td>\n",
       "      <td>0.138390</td>\n",
       "      <td>0.197292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0           BIOCARTA_GRANULOCYTES_PATHWAY  BIOCARTA_LYM_PATHWAY  \\\n",
       "Name                                                              \n",
       "ACH-001113                       0.201899              0.214778   \n",
       "ACH-000242                       0.210679              0.195089   \n",
       "ACH-000327                       0.226109              0.228415   \n",
       "ACH-000461                       0.230496              0.359472   \n",
       "ACH-000792                       0.394568              0.663835   \n",
       "\n",
       "0           BIOCARTA_BLYMPHOCYTE_PATHWAY  BIOCARTA_CARM_ER_PATHWAY  \\\n",
       "Name                                                                 \n",
       "ACH-001113                      0.107506                  0.436254   \n",
       "ACH-000242                      0.137566                  0.598515   \n",
       "ACH-000327                      0.131664                  0.807736   \n",
       "ACH-000461                      0.087643                  0.529038   \n",
       "ACH-000792                      0.308974                  0.400182   \n",
       "\n",
       "0           BIOCARTA_LAIR_PATHWAY  BIOCARTA_VDR_PATHWAY  \\\n",
       "Name                                                      \n",
       "ACH-001113               0.251323              0.543866   \n",
       "ACH-000242               0.192437              0.888515   \n",
       "ACH-000327               0.214956              0.105343   \n",
       "ACH-000461               0.244749              0.476536   \n",
       "ACH-000792               0.578346              0.287388   \n",
       "\n",
       "0           BIOCARTA_MTA3_PATHWAY  BIOCARTA_GABA_PATHWAY  \\\n",
       "Name                                                       \n",
       "ACH-001113               0.402230               0.446239   \n",
       "ACH-000242               0.381405               0.108177   \n",
       "ACH-000327               0.366447               0.534534   \n",
       "ACH-000461               0.201961               0.313338   \n",
       "ACH-000792               0.289313               0.165095   \n",
       "\n",
       "0           BIOCARTA_EGFR_SMRTE_PATHWAY  BIOCARTA_MONOCYTE_PATHWAY  ...  \\\n",
       "Name                                                                ...   \n",
       "ACH-001113                     0.692287                   0.299681  ...   \n",
       "ACH-000242                     0.679700                   0.285389  ...   \n",
       "ACH-000327                     0.508604                   0.279435  ...   \n",
       "ACH-000461                     0.724363                   0.298408  ...   \n",
       "ACH-000792                     0.557640                   0.630891  ...   \n",
       "\n",
       "0           HALLMARK_COAGULATION  HALLMARK_IL2_STAT5_SIGNALING  \\\n",
       "Name                                                             \n",
       "ACH-001113              0.369956                      0.589361   \n",
       "ACH-000242              0.448744                      0.651476   \n",
       "ACH-000327              0.377387                      0.627130   \n",
       "ACH-000461              0.475189                      0.725638   \n",
       "ACH-000792              0.624730                      0.829904   \n",
       "\n",
       "0           HALLMARK_BILE_ACID_METABOLISM  HALLMARK_PEROXISOME  \\\n",
       "Name                                                             \n",
       "ACH-001113                       0.231132             0.508302   \n",
       "ACH-000242                       0.461799             0.710828   \n",
       "ACH-000327                       0.639671             0.951057   \n",
       "ACH-000461                       0.126653             0.531472   \n",
       "ACH-000792                       0.180853             0.480622   \n",
       "\n",
       "0           HALLMARK_ALLOGRAFT_REJECTION  HALLMARK_SPERMATOGENESIS  \\\n",
       "Name                                                                 \n",
       "ACH-001113                      0.178205                  0.653705   \n",
       "ACH-000242                      0.337337                  0.380484   \n",
       "ACH-000327                      0.237352                  0.491974   \n",
       "ACH-000461                      0.348210                  0.451362   \n",
       "ACH-000792                      0.450919                  0.481082   \n",
       "\n",
       "0           HALLMARK_KRAS_SIGNALING  HALLMARK_KRAS_SIGNALING_UP  \\\n",
       "Name                                                              \n",
       "ACH-001113                 0.345327                    0.352541   \n",
       "ACH-000242                 0.503869                    0.462545   \n",
       "ACH-000327                 0.441105                    0.435839   \n",
       "ACH-000461                 0.611400                    0.517533   \n",
       "ACH-000792                 0.591145                    0.458782   \n",
       "\n",
       "0           HALLMARK_KRAS_SIGNALING_DN  HALLMARK_PANCREAS_BETA_CELLS  \n",
       "Name                                                                  \n",
       "ACH-001113                    0.390359                      0.232997  \n",
       "ACH-000242                    0.288830                      0.128070  \n",
       "ACH-000327                    0.353783                      0.339955  \n",
       "ACH-000461                    0.191123                      0.196242  \n",
       "ACH-000792                    0.138390                      0.197292  \n",
       "\n",
       "[5 rows x 3284 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter valid ID\n",
    "validSet = set()\n",
    "for name in trainData['Name']:\n",
    "    if CCLEidToCTRPNameDict[name] in cclToAUCdict:\n",
    "        validSet.add(name)\n",
    "trainData = trainData.loc[trainData['Name'].isin(validSet)]\n",
    "trainData = trainData.reset_index(drop = True)\n",
    "trainData = trainData.set_index(['Name'])\n",
    "trainData = trainData.apply(pd.to_numeric)\n",
    "trainData = normalize(trainData)\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90ce4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all(trainData.columns == predictData.columns):\n",
    "    raise Exception(\"Column do not match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53f9ca95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#labels data\n",
    "labelsDataOriginal = pd.DataFrame(columns = [sub[1] for sub in CTRPCCLAUC[7:]])\n",
    "for name in trainData.index:\n",
    "    labelsDataOriginal.loc[len(labelsDataOriginal.index)] = cclToAUCdict[CCLEidToCTRPNameDict[name]]\n",
    "labelsDataOriginal = labelsDataOriginal.set_index(trainData.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "651b49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add high priority at the begining. e.g dasatinib\n",
    "labels = list(labelsDataOriginal.columns)\n",
    "labels = ['dasatinib'] + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "062064dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constans\n",
    "problem_types = ['regression'] \n",
    "eval_metrics = ['mean_squared_error']\n",
    "time_limit = 60 * 60 * 24\n",
    "tops = [100,300,600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a029b871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 86400s\n",
      "AutoGluon will save models to \"dasatinib/GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Model_dasatinib/Predictor_dasatinib/\"\n",
      "AutoGluon Version:  0.8.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.4.0: Mon Mar  6 21:00:41 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   336.95 GB / 494.38 GB (68.2%)\n",
      "Train Data Rows:    619\n",
      "Train Data Columns: 3\n",
      "Label Column: dasatinib\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    426.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_LYM_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_LYM_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 495, Val Rows: 124\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 86399.98s of the 86399.97s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: dasatinib ...0.0%\n",
      "5.070368448893229e-06 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-6.2266\t = Validation score   (-mean_squared_error)\n",
      "\t4.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 86395.76s of the 86395.76s of remaining time.\n",
      "\t-6.5201\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 86395.76s of the 86395.76s of remaining time.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ... Training model for up to 86395.18s of the 86395.18s of remaining time.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ... Training model for up to 86395.16s of the 86395.16s of remaining time.\n",
      "\t-6.3401\t = Validation score   (-mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 86394.84s of the 86394.84s of remaining time.\n",
      "\t-5.5915\t = Validation score   (-mean_squared_error)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 86394.44s of the 86394.44s of remaining time.\n",
      "\t-6.1003\t = Validation score   (-mean_squared_error)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 86394.15s of the 86394.15s of remaining time.\n",
      "No improvement since epoch 5: early stopping\n",
      "\t-5.3097\t = Validation score   (-mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 86393.56s of the 86393.56s of remaining time.\n",
      "Warning: Large XGB model size may cause OOM error if training continues\n",
      "Available Memory: 363 MB\n",
      "Estimated XGB model size: 0 MB\n",
      "\t-78.9537\t = Validation score   (-mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 86393.53s of the 86393.52s of remaining time.\n",
      "\t-5.661\t = Validation score   (-mean_squared_error)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 86392.78s of the 86392.78s of remaining time.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 8640.0s of the 86392.77s of remaining time.\n",
      "\t-5.2621\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 7.33s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"dasatinib/GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Model_dasatinib/Predictor_dasatinib/\")\n",
      "Computing feature importance via permutation shuffling for 3 features using 619 rows with 3 shuffle sets...\n",
      "\t0.18s\t= Expected runtime (0.06s per shuffle set)\n",
      "\t0.1s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 86400s\n",
      "AutoGluon will save models to \"dasatinib/GlioML_ReducedFeature_100_dasatinib_FeatureEvaluationModel/Predictor_dasatinib/\"\n",
      "AutoGluon Version:  0.8.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.4.0: Mon Mar  6 21:00:41 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   336.94 GB / 494.38 GB (68.2%)\n",
      "Train Data Rows:    494\n",
      "Train Data Columns: 3\n",
      "Label Column: dasatinib\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    377.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 395, Val Rows: 99\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 86399.98s of the 86399.98s of remaining time.\n",
      "\t-7.6365\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 86399.97s of the 86399.97s of remaining time.\n",
      "\t-7.6944\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 86399.97s of the 86399.97s of remaining time.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ... Training model for up to 86399.96s of the 86399.96s of remaining time.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ... Training model for up to 86399.95s of the 86399.95s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('dasatinib/GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Model_dasatinib/')\n",
      "Predicting with TabularPredictor for label: dasatinib ...0.0%\n",
      "Fitting TabularPredictor for label: dasatinib ...0.0%\n",
      "4.915396372477214e-06 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-7.8006\t = Validation score   (-mean_squared_error)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 86399.68s of the 86399.68s of remaining time.\n",
      "\t-6.9472\t = Validation score   (-mean_squared_error)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 86399.48s of the 86399.48s of remaining time.\n",
      "\t-7.7263\t = Validation score   (-mean_squared_error)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 86399.23s of the 86399.23s of remaining time.\n",
      "No improvement since epoch 5: early stopping\n",
      "\t-7.2031\t = Validation score   (-mean_squared_error)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 86398.96s of the 86398.96s of remaining time.\n",
      "Warning: Large XGB model size may cause OOM error if training continues\n",
      "Available Memory: 360 MB\n",
      "Estimated XGB model size: 0 MB\n",
      "\t-79.6246\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 86398.89s of the 86398.89s of remaining time.\n",
      "\t-6.9414\t = Validation score   (-mean_squared_error)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 86398.1s of the 86398.1s of remaining time.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 8640.0s of the 86398.08s of remaining time.\n",
      "\t-6.8277\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.02s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"dasatinib/GlioML_ReducedFeature_100_dasatinib_FeatureEvaluationModel/Predictor_dasatinib/\")\n",
      "Evaluation: mean_squared_error on test data: -5.012023139305795\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"mean_squared_error\": -5.012023139305795,\n",
      "    \"root_mean_squared_error\": -2.2387548189352473,\n",
      "    \"mean_absolute_error\": -1.7885896258544922,\n",
      "    \"r2\": 0.16627205711744109,\n",
      "    \"pearsonr\": 0.44384103685263215,\n",
      "    \"median_absolute_error\": -1.5260121917724607\n",
      "}\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=5, num_bag_sets=2\n",
      "Beginning AutoGluon training ... Time limit = 86400s\n",
      "AutoGluon will save models to \"dasatinib/GiloML_predictDrugAUC_Top_100_Features_Best_Quality_Full_Data_Model_dasatinib/Predictor_dasatinib/\"\n",
      "AutoGluon Version:  0.8.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.4.0: Mon Mar  6 21:00:41 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   336.92 GB / 494.38 GB (68.1%)\n",
      "Train Data Rows:    619\n",
      "Train Data Columns: 3\n",
      "Label Column: dasatinib\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    371.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 86399.97s of the 86399.97s of remaining time.\n",
      "\t-7.0731\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 86399.95s of the 86399.95s of remaining time.\n",
      "\t-7.2264\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 86399.94s of the 86399.94s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('dasatinib/GlioML_ReducedFeature_100_dasatinib_FeatureEvaluationModel/')\n",
      "Evaluating TabularPredictor for label: dasatinib ...\n",
      "Fitting TabularPredictor for label: dasatinib ...0.0%\n",
      "4.680951436360677e-06 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=71406, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71406, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 86393.96s of the 86393.96s of remaining time.\n",
      "2023-07-04 22:52:42,332\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=71403, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71403, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "2023-07-04 22:52:42,334\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=71401, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71401, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "2023-07-04 22:52:42,351\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=71405, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71405, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "2023-07-04 22:52:42,355\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=71404, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71404, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=71402, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71402, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 86391.51s of the 86391.51s of remaining time.\n",
      "2023-07-04 22:52:44,809\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:52:44,837\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:52:44,838\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:52:44,839\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-6.8144\t = Validation score   (-mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 86391.14s of the 86391.14s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-6.1231\t = Validation score   (-mean_squared_error)\n",
      "\t1.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 86388.34s of the 86388.34s of remaining time.\n",
      "\t-6.7734\t = Validation score   (-mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 86387.9s of the 86387.9s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-5.9128\t = Validation score   (-mean_squared_error)\n",
      "\t2.88s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 86383.2s of the 86383.2s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-77.6213\t = Validation score   (-mean_squared_error)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 86379.92s of the 86379.91s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-5.8775\t = Validation score   (-mean_squared_error)\n",
      "\t3.11s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 86375.28s of the 86375.28s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=71473, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71473, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Repeating k-fold bagging: 2/2\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 86373.16s of the 86373.16s of remaining time.\n",
      "\tFitting 5 child models (S2F1 - S2F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-5.9869\t = Validation score   (-mean_squared_error)\n",
      "\t3.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 86370.17s of the 86370.17s of remaining time.\n",
      "\tFitting 5 child models (S2F1 - S2F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "2023-07-04 22:53:08,220\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:08,227\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:08,229\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:08,231\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-5.8383\t = Validation score   (-mean_squared_error)\n",
      "\t5.19s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 86366.1s of the 86366.1s of remaining time.\n",
      "\tFitting 5 child models (S2F1 - S2F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-77.6246\t = Validation score   (-mean_squared_error)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 86363.31s of the 86363.31s of remaining time.\n",
      "\tFitting 5 child models (S2F1 - S2F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-5.835\t = Validation score   (-mean_squared_error)\n",
      "\t5.87s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Completed 2/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 8640.0s of the 86359.02s of remaining time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.7925\t = Validation score   (-mean_squared_error)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 41.1s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"dasatinib/GiloML_predictDrugAUC_Top_100_Features_Best_Quality_Full_Data_Model_dasatinib/Predictor_dasatinib/\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 86400s\n",
      "AutoGluon will save models to \"dasatinib/GlioML_ReducedFeature_300_dasatinib_FeatureEvaluationModel/Predictor_dasatinib/\"\n",
      "AutoGluon Version:  0.8.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.4.0: Mon Mar  6 21:00:41 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   336.90 GB / 494.38 GB (68.1%)\n",
      "Train Data Rows:    494\n",
      "Train Data Columns: 3\n",
      "Label Column: dasatinib\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    430.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 395, Val Rows: 99\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 86399.98s of the 86399.98s of remaining time.\n",
      "\t-7.6365\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 86399.96s of the 86399.96s of remaining time.\n",
      "\t-7.6944\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 86399.95s of the 86399.95s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('dasatinib/GiloML_predictDrugAUC_Top_100_Features_Best_Quality_Full_Data_Model_dasatinib/')\n",
      "Predicting with TabularPredictor for label: dasatinib ...0.0%\n",
      "Fitting TabularPredictor for label: dasatinib ...0.0%\n",
      "7.335344950358073e-06 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ... Training model for up to 86399.93s of the 86399.93s of remaining time.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ... Training model for up to 86399.92s of the 86399.92s of remaining time.\n",
      "\t-7.8006\t = Validation score   (-mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 86399.66s of the 86399.66s of remaining time.\n",
      "\t-6.9472\t = Validation score   (-mean_squared_error)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 86399.44s of the 86399.44s of remaining time.\n",
      "\t-7.7263\t = Validation score   (-mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 86399.13s of the 86399.13s of remaining time.\n",
      "No improvement since epoch 5: early stopping\n",
      "\t-7.2031\t = Validation score   (-mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 86398.85s of the 86398.84s of remaining time.\n",
      "Warning: Large XGB model size may cause OOM error if training continues\n",
      "Available Memory: 484 MB\n",
      "Estimated XGB model size: 0 MB\n",
      "\t-79.6246\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 86398.83s of the 86398.83s of remaining time.\n",
      "\t-6.9414\t = Validation score   (-mean_squared_error)\n",
      "\t0.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 86398.01s of the 86398.01s of remaining time.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 8640.0s of the 86398.0s of remaining time.\n",
      "\t-6.8277\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.11s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"dasatinib/GlioML_ReducedFeature_300_dasatinib_FeatureEvaluationModel/Predictor_dasatinib/\")\n",
      "Evaluation: mean_squared_error on test data: -5.012023139305795\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"mean_squared_error\": -5.012023139305795,\n",
      "    \"root_mean_squared_error\": -2.2387548189352473,\n",
      "    \"mean_absolute_error\": -1.7885896258544922,\n",
      "    \"r2\": 0.16627205711744109,\n",
      "    \"pearsonr\": 0.44384103685263215,\n",
      "    \"median_absolute_error\": -1.5260121917724607\n",
      "}\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 86400s\n",
      "AutoGluon will save models to \"dasatinib/GlioML_ReducedFeature_600_dasatinib_FeatureEvaluationModel/Predictor_dasatinib/\"\n",
      "AutoGluon Version:  0.8.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.4.0: Mon Mar  6 21:00:41 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   336.88 GB / 494.38 GB (68.1%)\n",
      "Train Data Rows:    494\n",
      "Train Data Columns: 3\n",
      "Label Column: dasatinib\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    491.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('dasatinib/GlioML_ReducedFeature_300_dasatinib_FeatureEvaluationModel/')\n",
      "Evaluating TabularPredictor for label: dasatinib ...\n",
      "Fitting TabularPredictor for label: dasatinib ...0.0%\n",
      "6.46511713663737e-06 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\t0.3s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.32s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 395, Val Rows: 99\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 86399.68s of the 86399.68s of remaining time.\n",
      "\t-7.6365\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 86399.68s of the 86399.68s of remaining time.\n",
      "\t-7.6944\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 86399.67s of the 86399.67s of remaining time.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ... Training model for up to 86399.66s of the 86399.66s of remaining time.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ... Training model for up to 86399.65s of the 86399.65s of remaining time.\n",
      "\t-7.8006\t = Validation score   (-mean_squared_error)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 86399.38s of the 86399.38s of remaining time.\n",
      "\t-6.9472\t = Validation score   (-mean_squared_error)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 86399.2s of the 86399.19s of remaining time.\n",
      "\t-7.7263\t = Validation score   (-mean_squared_error)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 86398.94s of the 86398.94s of remaining time.\n",
      "No improvement since epoch 5: early stopping\n",
      "\t-7.2031\t = Validation score   (-mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 86398.67s of the 86398.66s of remaining time.\n",
      "Warning: Large XGB model size may cause OOM error if training continues\n",
      "Available Memory: 420 MB\n",
      "Estimated XGB model size: 0 MB\n",
      "\t-79.6246\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 86398.65s of the 86398.65s of remaining time.\n",
      "\t-6.9414\t = Validation score   (-mean_squared_error)\n",
      "\t0.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 86397.83s of the 86397.83s of remaining time.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 8639.97s of the 86397.82s of remaining time.\n",
      "\t-6.8277\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.29s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"dasatinib/GlioML_ReducedFeature_600_dasatinib_FeatureEvaluationModel/Predictor_dasatinib/\")\n",
      "Evaluation: mean_squared_error on test data: -5.012023139305795\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"mean_squared_error\": -5.012023139305795,\n",
      "    \"root_mean_squared_error\": -2.2387548189352473,\n",
      "    \"mean_absolute_error\": -1.7885896258544922,\n",
      "    \"r2\": 0.16627205711744109,\n",
      "    \"pearsonr\": 0.44384103685263215,\n",
      "    \"median_absolute_error\": -1.5260121917724607\n",
      "}\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=5, num_bag_sets=2\n",
      "Beginning AutoGluon training ... Time limit = 86400s\n",
      "AutoGluon will save models to \"dasatinib/GiloML_predictDrugAUC_Top_300_Features_Best_Quality_Full_Data_Model_dasatinib/Predictor_dasatinib/\"\n",
      "AutoGluon Version:  0.8.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.4.0: Mon Mar  6 21:00:41 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   336.86 GB / 494.38 GB (68.1%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Data Rows:    619\n",
      "Train Data Columns: 3\n",
      "Label Column: dasatinib\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    439.8 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_LYM_PATHWAY', 'BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 86399.98s of the 86399.98s of remaining time.\n",
      "\t-7.0731\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 86399.97s of the 86399.96s of remaining time.\n",
      "\t-7.2264\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 86399.96s of the 86399.95s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('dasatinib/GlioML_ReducedFeature_600_dasatinib_FeatureEvaluationModel/')\n",
      "Evaluating TabularPredictor for label: dasatinib ...\n",
      "Fitting TabularPredictor for label: dasatinib ...0.0%\n",
      "4.99884287516276e-06 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=71519, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71519, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 86397.89s of the 86397.89s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=71535, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71535, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 86395.99s of the 86395.98s of remaining time.\n",
      "2023-07-04 22:53:26,002\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:26,028\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:26,045\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:26,046\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:26,051\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:26,058\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:26,061\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 22:53:26,067\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-6.8144\t = Validation score   (-mean_squared_error)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 86395.59s of the 86395.59s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-6.1231\t = Validation score   (-mean_squared_error)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 86392.94s of the 86392.93s of remaining time.\n",
      "\t-6.7734\t = Validation score   (-mean_squared_error)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 86392.54s of the 86392.54s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-5.9128\t = Validation score   (-mean_squared_error)\n",
      "\t2.26s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 86388.24s of the 86388.24s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-77.6213\t = Validation score   (-mean_squared_error)\n",
      "\t0.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 86385.16s of the 86385.16s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-5.8775\t = Validation score   (-mean_squared_error)\n",
      "\t2.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 86380.7s of the 86380.7s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=71583, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71583, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Repeating k-fold bagging: 2/2\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 86378.73s of the 86378.73s of remaining time.\n",
      "\tFitting 5 child models (S2F1 - S2F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-5.9869\t = Validation score   (-mean_squared_error)\n",
      "\t2.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 86375.63s of the 86375.63s of remaining time.\n",
      "\tFitting 5 child models (S2F1 - S2F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "2023-07-04 22:53:48,320\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:48,323\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:48,325\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:53:48,326\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-5.8383\t = Validation score   (-mean_squared_error)\n",
      "\t4.65s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 86371.49s of the 86371.49s of remaining time.\n",
      "\tFitting 5 child models (S2F1 - S2F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-77.6246\t = Validation score   (-mean_squared_error)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 86368.74s of the 86368.74s of remaining time.\n",
      "\tFitting 5 child models (S2F1 - S2F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-5.835\t = Validation score   (-mean_squared_error)\n",
      "\t5.99s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Completed 2/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 8640.0s of the 86363.65s of remaining time.\n",
      "\t-5.7925\t = Validation score   (-mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 36.49s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"dasatinib/GiloML_predictDrugAUC_Top_300_Features_Best_Quality_Full_Data_Model_dasatinib/Predictor_dasatinib/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('dasatinib/GiloML_predictDrugAUC_Top_300_Features_Best_Quality_Full_Data_Model_dasatinib/')\n",
      "Predicting with TabularPredictor for label: dasatinib ...0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 86400s\n",
      "AutoGluon will save models to \"zebularine/GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Model_zebularine/Predictor_zebularine/\"\n",
      "AutoGluon Version:  0.8.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.4.0: Mon Mar  6 21:00:41 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   336.82 GB / 494.38 GB (68.1%)\n",
      "Train Data Rows:    615\n",
      "Train Data Columns: 3\n",
      "Label Column: zebularine\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    350.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_LYM_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_LYM_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\t0.1s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: zebularine ...0.0%\n",
      "2.353191375732422e-05 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 492, Val Rows: 123\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 86399.84s of the 86399.84s of remaining time.\n",
      "\t-1.0084\t = Validation score   (-mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 86399.77s of the 86399.77s of remaining time.\n",
      "\t-1.031\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 86399.76s of the 86399.76s of remaining time.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ... Training model for up to 86399.74s of the 86399.73s of remaining time.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ... Training model for up to 86399.72s of the 86399.72s of remaining time.\n",
      "\t-1.0327\t = Validation score   (-mean_squared_error)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 86399.43s of the 86399.43s of remaining time.\n",
      "\t-0.8404\t = Validation score   (-mean_squared_error)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 86399.22s of the 86399.22s of remaining time.\n",
      "\t-0.9202\t = Validation score   (-mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 86398.96s of the 86398.96s of remaining time.\n",
      "\t-0.8345\t = Validation score   (-mean_squared_error)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 86398.62s of the 86398.62s of remaining time.\n",
      "Warning: Large XGB model size may cause OOM error if training continues\n",
      "Available Memory: 414 MB\n",
      "Estimated XGB model size: 0 MB\n",
      "\t-156.324\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 86398.61s of the 86398.6s of remaining time.\n",
      "\t-0.8349\t = Validation score   (-mean_squared_error)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 86397.42s of the 86397.42s of remaining time.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 8639.98s of the 86397.41s of remaining time.\n",
      "\t-0.8295\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"zebularine/GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Model_zebularine/Predictor_zebularine/\")\n",
      "Computing feature importance via permutation shuffling for 3 features using 615 rows with 3 shuffle sets...\n",
      "\t0.68s\t= Expected runtime (0.23s per shuffle set)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('zebularine/GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Model_zebularine/')\n",
      "Predicting with TabularPredictor for label: zebularine ...0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.3s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 86400s\n",
      "AutoGluon will save models to \"zebularine/GlioML_ReducedFeature_100_zebularine_FeatureEvaluationModel/Predictor_zebularine/\"\n",
      "AutoGluon Version:  0.8.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.4.0: Mon Mar  6 21:00:41 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   336.79 GB / 494.38 GB (68.1%)\n",
      "Train Data Rows:    489\n",
      "Train Data Columns: 3\n",
      "Label Column: zebularine\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    390.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_LYM_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_LYM_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 391, Val Rows: 98\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 86399.98s of the 86399.98s of remaining time.\n",
      "\t-1.1885\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 86399.97s of the 86399.97s of remaining time.\n",
      "\t-1.2276\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 86399.97s of the 86399.97s of remaining time.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ... Training model for up to 86399.96s of the 86399.96s of remaining time.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ... Training model for up to 86399.94s of the 86399.94s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: zebularine ...0.0%\n",
      "3.898143768310547e-06 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0889\t = Validation score   (-mean_squared_error)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 86399.67s of the 86399.67s of remaining time.\n",
      "\t-0.944\t = Validation score   (-mean_squared_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 86399.51s of the 86399.51s of remaining time.\n",
      "\t-1.0707\t = Validation score   (-mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 86399.25s of the 86399.25s of remaining time.\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-0.9376\t = Validation score   (-mean_squared_error)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 86398.94s of the 86398.94s of remaining time.\n",
      "Warning: Large XGB model size may cause OOM error if training continues\n",
      "Available Memory: 390 MB\n",
      "Estimated XGB model size: 0 MB\n",
      "\t-158.4917\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 86398.91s of the 86398.91s of remaining time.\n",
      "\t-0.9301\t = Validation score   (-mean_squared_error)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 86397.95s of the 86397.95s of remaining time.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 8640.0s of the 86397.94s of remaining time.\n",
      "\t-0.9287\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"zebularine/GlioML_ReducedFeature_100_zebularine_FeatureEvaluationModel/Predictor_zebularine/\")\n",
      "Evaluation: mean_squared_error on test data: -0.9803229507377887\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"mean_squared_error\": -0.9803229507377887,\n",
      "    \"root_mean_squared_error\": -0.9901125949798782,\n",
      "    \"mean_absolute_error\": -0.712776628524538,\n",
      "    \"r2\": -0.0031897046362046044,\n",
      "    \"pearsonr\": 0.062466229299814935,\n",
      "    \"median_absolute_error\": -0.5070482635498053\n",
      "}\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=5, num_bag_sets=2\n",
      "Beginning AutoGluon training ... Time limit = 86400s\n",
      "AutoGluon will save models to \"zebularine/GiloML_predictDrugAUC_Top_100_Features_Best_Quality_Full_Data_Model_zebularine/Predictor_zebularine/\"\n",
      "AutoGluon Version:  0.8.0\n",
      "Python Version:     3.9.7\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.4.0: Mon Mar  6 21:00:41 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   336.77 GB / 494.38 GB (68.1%)\n",
      "Train Data Rows:    615\n",
      "Train Data Columns: 3\n",
      "Label Column: zebularine\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    414.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_LYM_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['BIOCARTA_GRANULOCYTES_PATHWAY', 'BIOCARTA_LYM_PATHWAY', 'BIOCARTA_BLYMPHOCYTE_PATHWAY']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 86399.98s of the 86399.98s of remaining time.\n",
      "\t-1.3284\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 86399.96s of the 86399.96s of remaining time.\n",
      "\t-1.3724\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 86399.95s of the 86399.95s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('zebularine/GlioML_ReducedFeature_100_zebularine_FeatureEvaluationModel/')\n",
      "Evaluating TabularPredictor for label: zebularine ...\n",
      "Fitting TabularPredictor for label: zebularine ...0.0%\n",
      "4.851818084716797e-06 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=71635, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71635, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 86397.9s of the 86397.89s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "2023-07-04 22:54:08,125\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=71676, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71676, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "2023-07-04 22:54:08,137\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 86395.98s of the 86395.96s of remaining time.\n",
      "2023-07-04 22:54:08,170\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:54:08,174\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-1.3256\t = Validation score   (-mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 86395.59s of the 86395.59s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-1.0704\t = Validation score   (-mean_squared_error)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 86392.93s of the 86392.93s of remaining time.\n",
      "2023-07-04 22:54:11,279\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 22:54:11,286\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:54:11,290\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:54:11,297\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-1.2675\t = Validation score   (-mean_squared_error)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 86392.48s of the 86392.48s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-1.0519\t = Validation score   (-mean_squared_error)\n",
      "\t2.45s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 86388.53s of the 86388.53s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-158.4752\t = Validation score   (-mean_squared_error)\n",
      "\t0.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 86385.56s of the 86385.56s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-1.0563\t = Validation score   (-mean_squared_error)\n",
      "\t3.44s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 86379.91s of the 86379.91s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=71724, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset, Sequence, register_logger\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\", line 101, in _load_lib\n",
      "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=71724, ip=127.0.0.1)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 394, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 811, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 82, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "  File \"/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/autogluon/common/utils/try_import.py\", line 140, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. If you are using Mac OSX, \"\n",
      "ImportError: `import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/moonr_-/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Repeating k-fold bagging: 2/2\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 86377.26s of the 86377.26s of remaining time.\n",
      "\tFitting 5 child models (S2F1 - S2F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "2023-07-04 22:54:26,902\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:54:26,905\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:54:26,905\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-07-04 22:54:26,906\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y4/nb52fxtx6g934cctxtd11mz40000gn/T/ipykernel_71342/3020725767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0msave_path_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'GiloML_predictDrugAUC_Top_100_Features_Best_Quality_Full_Data_Model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mpredictor_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultilabelPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_path_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mpredictor_single\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDataSetR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mresult_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor_single\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreducedfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mresult_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path_final\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_Result.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/y4/nb52fxtx6g934cctxtd11mz40000gn/T/ipykernel_71342/3564636717.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, tuning_data, final, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{(time.time() - start) / 60} minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 predictor.fit(train_data=train_data[train_data[label] > float('-inf')]\n\u001b[0m\u001b[1;32m     90\u001b[0m                               \u001b[0;34m,\u001b[0m \u001b[0mtuning_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuning_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                               \u001b[0;34m,\u001b[0m\u001b[0mpresets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'best_quality'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/utils/decorators.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mother_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_inner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0maux_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fit_weighted_ensemble'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save predictor to disk to enable prediction and training after interrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m         self._learner.fit(X=train_data, X_val=tuning_data, X_unlabeled=unlabeled_data,\n\u001b[0m\u001b[1;32m    952\u001b[0m                           \u001b[0mholdout_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mholdout_frac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bag_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_bag_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bag_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_bag_sets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                           \u001b[0mnum_stack_levels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_stack_levels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/learner/abstract_learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Learner is already fit.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_fit_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     def _fit(self, X: DataFrame, X_val: DataFrame = None, scheduler_options=None, hyperparameter_tune=False,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/learner/default_learner.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         trainer.fit(\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/tabular/trainer/auto_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         self._train_multi_and_ensemble(X=X,\n\u001b[0m\u001b[1;32m    107\u001b[0m                                        \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                                        \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2091\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_rows_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_cols_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2093\u001b[0;31m         model_names_fit = self.train_multi_levels(X, y, hyperparameters=hyperparameters, X_val=X_val, y_val=y_val,\n\u001b[0m\u001b[1;32m   2094\u001b[0m                                                   X_unlabeled=X_unlabeled, level_start=1, level_end=num_stack_levels+1, time_limit=time_limit, **kwargs)\n\u001b[1;32m   2095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36mtrain_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mcore_kwargs_level\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_limit'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore_kwargs_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_limit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit_core\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0maux_kwargs_level\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_limit'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_kwargs_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_limit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit_aux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             base_model_names, aux_models = self.stack_new_level(\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_model_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36mstack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mcore_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name_suffix'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name_suffix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0maux_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name_suffix'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name_suffix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         core_models = self.stack_new_level_core(X=X, y=y, X_val=X_val, y_val=y_val, X_unlabeled=X_unlabeled, models=models,\n\u001b[0m\u001b[1;32m    438\u001b[0m                                                 level=level, infer_limit=infer_limit, infer_limit_batch_size=infer_limit_batch_size, base_model_names=base_model_names, **core_kwargs)\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36mstack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         return self._train_multi(X=X_init, y=y, X_val=X_val, y_val=y_val, X_unlabeled=X_unlabeled,\n\u001b[0m\u001b[1;32m    549\u001b[0m                                  models=models, level=level, stack_name=stack_name, compute_score=compute_score, fit_kwargs=fit_kwargs, **kwargs)\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2069\u001b[0m             \u001b[0mmodel_names_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_repeats\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_repeat_start\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2071\u001b[0;31m             model_names_trained = self._train_multi_repeats(X=X, y=y, models=model_names_trained,\n\u001b[0m\u001b[1;32m   2072\u001b[0m                                                             k_fold=k_fold, n_repeats=n_repeats, n_repeat_start=n_repeat_start, time_limit=time_limit, time_limit_total_level=time_limit_total_level, **kwargs)\n\u001b[1;32m   2073\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_names_trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi_repeats\u001b[0;34m(self, X, y, models, n_repeats, n_repeat_start, time_limit, time_limit_total_level, **kwargs)\u001b[0m\n\u001b[1;32m   1916\u001b[0m                     \u001b[0mtime_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime_start_model\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m                 \u001b[0mmodels_valid_next\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_single_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeat_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m             \u001b[0mmodels_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_valid_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0mmodels_valid_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[1;32m   1850\u001b[0m                 \u001b[0mbagged_model_fit_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_bagged_model_fit_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_fold_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_fold_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeat_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeat_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m                 \u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbagged_model_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m             model_names_trained = self._train_and_save(\n\u001b[0m\u001b[1;32m   1853\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_and_save\u001b[0;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1542\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_w_pseudo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_w_pseudo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_resources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_resources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m             \u001b[0mfit_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_single\u001b[0;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \"\"\"\n\u001b[0;32m-> 1485\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_resources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_resources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_fit_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_fit_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mtime_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m                     \u001b[0;31m# Reserve time for final refit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_limit'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_limit'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfolds_to_fit\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfolds_to_fit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n\u001b[0m\u001b[1;32m    257\u001b[0m                             \u001b[0mk_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_fold_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_fold_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                             n_repeats=n_repeats, n_repeat_start=n_repeat_start, save_folds=save_bag_folds, groups=groups, **kwargs)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\u001b[0m in \u001b[0;36m_fit_folds\u001b[0;34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold_fit_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfold_fit_args_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mfold_fitting_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule_fold_model_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfold_fit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mfold_fitting_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_all_folds_scheduled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\u001b[0m in \u001b[0;36mafter_all_folds_scheduled\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0munfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mfinished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfinished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_returns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2577\u001b[0m         \u001b[0mtimeout_milliseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2578\u001b[0;31m         ready_ids, remaining_ids = worker.core_worker.wait(\n\u001b[0m\u001b[1;32m   2579\u001b[0m             \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    if len(os.listdir(label)) > 8:\n",
    "         continue\n",
    "    \n",
    "    labelData = labelsDataOriginal[[label]]    \n",
    "    trainDataSet = pd.concat([trainData, labelData], axis = 1)\n",
    "    \n",
    "    #first time training \n",
    "    save_path = label + '/' + 'GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Model_' + label \n",
    "    multi_predictor = MultilabelPredictor(labels=[label], problem_types=problem_types, eval_metrics=eval_metrics, path=save_path)\n",
    "    #multi_predictor.fit(trainDataSet, time_limit=time_limit)\n",
    "    multi_predictor = MultilabelPredictor.load(save_path)\n",
    "    result = multi_predictor.predict(predictData)\n",
    "    result.to_csv(label + '/' + 'GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Result_' + label+ '.csv')\n",
    "    \n",
    "    #get feature importance\n",
    "    predictor = multi_predictor.get_predictor(label)\n",
    "    feature_importance = predictor.feature_importance(trainDataSet[trainDataSet[label] > float('-inf')], num_shuffle_sets = 3)\n",
    "    feature_importance.to_csv(label + '/' + \"GlioML_feature_importance_\" + label + \".csv\")\n",
    "    \n",
    "    #only consider feature has positive contribution\n",
    "    feature_importance = feature_importance[feature_importance['importance'] > 0]\n",
    "    \n",
    "    #Evaluation\n",
    "    evaaluateResult = []\n",
    "    for top in tops:\n",
    "        reducedfeatures = list(feature_importance[:top].index)\n",
    "        trainDataR =  trainData[reducedfeatures]\n",
    "        trainDataSetR = pd.concat([trainDataR,labelData],axis = 1)\n",
    "        save_path_R = label + '/' + 'GlioML_ReducedFeature_' + str(top) + '_' + label + '_FeatureEvaluationModel'\n",
    "        train, evaldata = train_test_split(trainDataSetR,test_size = 0.2,random_state=1112)\n",
    "        predictor_single = MultilabelPredictor(labels=[label], problem_types=problem_types, eval_metrics=eval_metrics, path=save_path_R)\n",
    "        predictor_single.fit(train, time_limit=time_limit)\n",
    "        # predictor_single = MultilabelPredictor.load(save_path_R)\n",
    "        evaluationData = predictor_single.evaluate(evaldata)\n",
    "        evaaluateResult.append(evaluationData[label]['mean_squared_error'])\n",
    "        if(top == 100):\n",
    "            save_path_final = label + '/' + 'GiloML_predictDrugAUC_Top_100_Features_Best_Quality_Full_Data_Model_' + label\n",
    "            predictor_single = MultilabelPredictor(labels = [label], problem_types = problem_types, eval_metrics = eval_metrics, path = save_path_final)\n",
    "            predictor_single.fit(trainDataSetR, final = True, time_limit = time_limit)\n",
    "            result_final = predictor_single.predict(predictData[reducedfeatures])\n",
    "            result_final.to_csv(save_path_final + \"_Result.csv\")\n",
    "    \n",
    "    maxPerformanceFeatureCount = tops[evaaluateResult.index(max(evaaluateResult))]\n",
    "    evaaluateResult = [['Component Name','TOP100','TOP300','TOP600'], [label] + evaaluateResult]\n",
    "    with open(label + '/' + 'GlioML_ReducedFeature_evalScore_' + label + '.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(evaaluateResult)\n",
    "    \n",
    "    if maxPerformanceFeatureCount > 100: \n",
    "        reducedfeatures = list(feature_importance[:maxPerformanceFeatureCount].index)\n",
    "        trainDataR =  trainData[reducedfeatures]\n",
    "        trainDataSetR = pd.concat([trainDataR,labelData],axis = 1)\n",
    "        save_path_final = label + '/' + 'GiloML_predictDrugAUC_Top_' + str(maxPerformanceFeatureCount) +'_Features_Best_Quality_Full_Data_Model_' + label\n",
    "        predictor_single = MultilabelPredictor(labels = [label], problem_types = problem_types, eval_metrics = eval_metrics, path = save_path_final)\n",
    "        predictor_single.fit(trainDataSetR, final = True, time_limit = time_limit)\n",
    "        result_final = predictor_single.predict(predictData[reducedfeatures])\n",
    "        result_final.to_csv(save_path_final + \"_Result.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56250e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"dasatinib/GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Model_dasatinib/Predictor_dasatinib\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with TabularPredictor for label: dasatinib ...0.0%\n"
     ]
    }
   ],
   "source": [
    "model_name = 'GiloML_predictDrugAUC_Full_Feature_Medium_Quality_Model_'\n",
    "final_result = pd.DataFrame()\n",
    "for label in ['dasatinib']:\n",
    "    save_path = label + '/' + model_name + label \n",
    "    multi_predictor = MultilabelPredictor(labels=[label], problem_types=problem_types, eval_metrics=eval_metrics, path=save_path)\n",
    "    multi_predictor = MultilabelPredictor.load(save_path)\n",
    "    result = multi_predictor.predict(predictData)\n",
    "    final_result[label] = result[label]\n",
    "final_result.to_csv(model_name + 'Result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7fc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10fcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673013cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a349e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
